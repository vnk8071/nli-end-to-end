{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:29:05.535418Z","iopub.status.busy":"2022-09-29T08:29:05.534970Z","iopub.status.idle":"2022-09-29T08:30:36.170996Z","shell.execute_reply":"2022-09-29T08:30:36.170004Z","shell.execute_reply.started":"2022-09-29T08:29:05.535321Z"},"trusted":true},"outputs":[],"source":["!pip install torchtext==0.11.0\n","!pip install torch==1.10.0\n","!pip install -U deep_translator"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:36.174068Z","iopub.status.busy":"2022-09-29T08:30:36.173651Z","iopub.status.idle":"2022-09-29T08:30:42.350017Z","shell.execute_reply":"2022-09-29T08:30:42.349124Z","shell.execute_reply.started":"2022-09-29T08:30:36.174021Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>premise</th>\n","      <th>hypothesis</th>\n","      <th>lang_abv</th>\n","      <th>language</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5130fd2cb5</td>\n","      <td>and these comments were considered in formulat...</td>\n","      <td>The rules developed in the interim were put to...</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5b72532a0b</td>\n","      <td>These are issues that we wrestle with in pract...</td>\n","      <td>Practice groups are not permitted to work on t...</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3931fbe82a</td>\n","      <td>Des petites choses comme celles-là font une di...</td>\n","      <td>J'essayais d'accomplir quelque chose.</td>\n","      <td>fr</td>\n","      <td>French</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5622f0c60b</td>\n","      <td>you know they can't really defend themselves l...</td>\n","      <td>They can't defend themselves because of their ...</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>86aaa48b45</td>\n","      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n","      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n","      <td>th</td>\n","      <td>Thai</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12115</th>\n","      <td>2b78e2a914</td>\n","      <td>The results of even the most well designed epi...</td>\n","      <td>All studies have the same amount of uncertaint...</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12116</th>\n","      <td>7e9943d152</td>\n","      <td>But there are two kinds of  the pleasure of do...</td>\n","      <td>But there are two kinds of the pleasure of doi...</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12117</th>\n","      <td>5085923e6c</td>\n","      <td>The important thing is to realize that it's wa...</td>\n","      <td>It cannot be moved, now or ever.</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12118</th>\n","      <td>fc8e2fd1fe</td>\n","      <td>At the west end is a detailed model of the who...</td>\n","      <td>The model temple complex is at the east end.</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12119</th>\n","      <td>44301dfb14</td>\n","      <td>For himself he chose Atat??rk, or Father of th...</td>\n","      <td>Ataturk was the father of the Turkish nation.</td>\n","      <td>en</td>\n","      <td>English</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12120 rows × 6 columns</p>\n","</div>"],"text/plain":["               id                                            premise  \\\n","0      5130fd2cb5  and these comments were considered in formulat...   \n","1      5b72532a0b  These are issues that we wrestle with in pract...   \n","2      3931fbe82a  Des petites choses comme celles-là font une di...   \n","3      5622f0c60b  you know they can't really defend themselves l...   \n","4      86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n","...           ...                                                ...   \n","12115  2b78e2a914  The results of even the most well designed epi...   \n","12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n","12117  5085923e6c  The important thing is to realize that it's wa...   \n","12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n","12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n","\n","                                              hypothesis lang_abv language  \\\n","0      The rules developed in the interim were put to...       en  English   \n","1      Practice groups are not permitted to work on t...       en  English   \n","2                  J'essayais d'accomplir quelque chose.       fr   French   \n","3      They can't defend themselves because of their ...       en  English   \n","4        เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai   \n","...                                                  ...      ...      ...   \n","12115  All studies have the same amount of uncertaint...       en  English   \n","12116  But there are two kinds of the pleasure of doi...       en  English   \n","12117                   It cannot be moved, now or ever.       en  English   \n","12118       The model temple complex is at the east end.       en  English   \n","12119      Ataturk was the father of the Turkish nation.       en  English   \n","\n","       label  \n","0          0  \n","1          2  \n","2          0  \n","3          0  \n","4          1  \n","...      ...  \n","12115      2  \n","12116      0  \n","12117      2  \n","12118      2  \n","12119      0  \n","\n","[12120 rows x 6 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Custom BERT model (https://www.kaggle.com/code/shineiarakawa/pytorch-bert-by-transformers)\n","# Feature engineering basic (https://github.com/codemunic/Natural-Language-Inference/blob/main/bert_nli_pytorch.ipynb)\n","\n","import os\n","import re\n","import math\n","import time\n","import datetime\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.legacy import data\n","from transformers import BertTokenizer, BertModel, BertConfig, AdamW, get_constant_schedule_with_warmup\n","from deep_translator import GoogleTranslator\n","from dask import bag, diagnostics\n","\n","SEED = 8071\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","df_train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\n","df_train"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:42.352351Z","iopub.status.busy":"2022-09-29T08:30:42.351974Z","iopub.status.idle":"2022-09-29T08:30:42.371355Z","shell.execute_reply":"2022-09-29T08:30:42.370370Z","shell.execute_reply.started":"2022-09-29T08:30:42.352312Z"},"trusted":true},"outputs":[],"source":["class FeatureEngineeringNLI:\n","\n","    TRIM_CHARACTER = 128\n","\n","    def __init__(self, df_path: str, is_train: bool):\n","        self.df_path = df_path\n","        self.df = pd.read_csv(self.df_path)\n","        self.is_train = is_train\n","        self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","\n","    def tokenize_bert(self, sentence):\n","        tokens = self.tokenizer.tokenize(sentence)\n","        return tokens\n","    \n","    def trim_sentence(self, sentence):\n","        try:\n","            sent = re.sub(r'\\(|\\)|\\[|\\]|\\{|\\}', '', sentence)\n","            sent = sent.split()\n","            sent = sent[:self.TRIM_CHARACTER]\n","            return \" \".join(sent)\n","        except:\n","            return sentence\n","    \n","    def premise_token_type(self, sentence):\n","        return [0] * len(sentence)\n","    \n","    def hypothesis_token_type(self, sentence):\n","        return [1] * len(sentence)\n","\n","    def combine_sequence(self, sentence):\n","        return \" \".join(sentence)\n","\n","    def combine_mask(self, mask):\n","        mask = [str(m) for m in mask]\n","        return \" \".join(mask)\n","    \n","    #TODO: use a dask dataframe instead of all this\n","    def trans_parallel(self, df, translator):\n","        premise_bag = bag.from_sequence(df.premise.tolist()).map(translator.translate)\n","        hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(translator.translate)\n","        with diagnostics.ProgressBar():\n","            premises = premise_bag.compute()\n","            hypos = hypo_bag.compute()\n","        df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n","        return df\n","    \n","    def english_translate(self, df):\n","        translator = GoogleTranslator(target='en')\n","        df[df.lang_abv != \"en\"] =  df.loc[df.lang_abv != \"en\"].copy().pipe(self.trans_parallel, translator)\n","        df['lang_abv'] = ['en']*len(df)\n","        df['language'] = ['English']*len(df)\n","        return df\n","    \n","    def process_data(self):\n","        self.df = self.english_translate(self.df)\n","        self.df['premise_trim'] = self.df['premise'].apply(self.trim_sentence)\n","        self.df['hypothesis_trim'] = self.df['hypothesis'].apply(self.trim_sentence)\n","\n","        self.df['premise_format'] = '[CLS] '  + self.df['premise_trim'] + ' [SEP] '\n","        self.df['hypothesis_format'] = self.df['hypothesis_trim'] + ' [SEP]'\n","\n","        self.df['premise_tokenizer'] = self.df['premise_format'].apply(self.tokenize_bert)\n","        self.df['hypothesis_tokenizer'] = self.df['hypothesis_format'].apply(self.tokenize_bert)\n","        self.df['sequence'] = self.df['premise_tokenizer'] + self.df['hypothesis_tokenizer']\n","\n","        self.df['premise_token_type'] = self.df['premise_tokenizer'].apply(self.premise_token_type)\n","        self.df['hypothesis_token_type'] = self.df['hypothesis_tokenizer'].apply(self.hypothesis_token_type)\n","        self.df['token_type'] = self.df['premise_token_type'] + self.df['hypothesis_token_type']\n","\n","        self.df['attention_mask'] = self.df['sequence'].apply(self.hypothesis_token_type)\n","        self.df['attention_mask'] = self.df['attention_mask'].apply(self.combine_mask)\n","        self.df['token_type'] = self.df['token_type'].apply(self.combine_mask)\n","        self.df['sequence'] = self.df['sequence'].apply(self.combine_sequence)\n","        \n","        if self.is_train:\n","            self.df = self.df[['label', 'sequence', 'attention_mask', 'token_type']]\n","            random_state = np.random.RandomState()\n","            train = self.df.sample(frac=0.8, random_state=random_state)\n","            train.to_csv('train_feature_engineering.csv', index=False)\n","            valid = self.df.loc[~self.df.index.isin(train.index)]\n","            valid.to_csv('valid_feature_engineering.csv', index=False)\n","        else:\n","            self.df = self.df[['sequence', 'attention_mask', 'token_type']]\n","            self.df.to_csv('test_feature_engineering.csv', index=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:42.373648Z","iopub.status.busy":"2022-09-29T08:30:42.373291Z","iopub.status.idle":"2022-09-29T08:30:42.390215Z","shell.execute_reply":"2022-09-29T08:30:42.389376Z","shell.execute_reply.started":"2022-09-29T08:30:42.373613Z"},"trusted":true},"outputs":[],"source":["class BERTNLIModel(nn.Module):\n","    def __init__(self, output_dim, dropout_rate):\n","        super(BERTNLIModel, self).__init__()\n","        self.bertnli_model = SingleBERT(use_pooling=False)\n","        self.bert_config = self.bertnli_model.get_config()\n","        self.hidden_size = self.bert_config.hidden_size\n","        self.output_dim = output_dim\n","        self.classifier = Classifier(hidden_size=self.hidden_size, num_classes=self.output_dim, dropout_rate=dropout_rate)\n","\n","    def forward(self, sequence, attention_mask, token_type):\n","        output = self.bertnli_model(input=sequence, attention_mask=attention_mask, token_type_ids=token_type)\n","        output = self.classifier(output)\n","        return output\n","\n","    def count_parameters(self):\n","        total_params = sum(p.numel() for p in self.bertnli_model.parameters() if p.requires_grad)\n","        return print(f'The model has {total_params:,} trainable parameters')\n","\n","class SingleBERT(nn.Module):\n","    def __init__(self, use_pooling=False):\n","        super(SingleBERT, self).__init__()\n","        self.use_pooling = use_pooling\n","        self.bert_config = BertConfig.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","        self.bert_model = BertModel.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","\n","        for param in self.bert_model.parameters():\n","            param.requires_grad = True\n","\n","    def forward(self, input, attention_mask, token_type_ids):\n","        last_layer_output, pooling_output = self.bert_model(input, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n","        if self.use_pooling:\n","            return pooling_output\n","        return last_layer_output[:, 0, :]\n","\n","    def get_config(self):\n","        return self.bert_config\n","\n","        \n","class Classifier(nn.Module):\n","    def __init__(self, hidden_size, num_classes, dropout_rate):\n","        super(Classifier, self).__init__()\n","        self.dropout1 = nn.Dropout(p=dropout_rate)\n","        self.linear1 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n","        self.batchNorm = nn.BatchNorm1d(num_features=hidden_size, eps=1e-05, momentum=0.1, affine=False)\n","        self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        self.dropout2 = nn.Dropout(p=dropout_rate)\n","        self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_classes)\n","\n","        nn.init.normal_(self.linear1.weight, std=0.04)\n","        nn.init.normal_(self.linear2.weight, mean=0.5, std=0.04)\n","        nn.init.normal_(self.linear1.bias, 0)\n","        nn.init.normal_(self.linear2.bias, 0)\n","\n","    def forward(self, input):\n","        output = self.dropout1(input)\n","        output = self.linear1(output)\n","        output = self.batchNorm(output)\n","        output = self.activation(output)\n","        output = self.dropout2(output)\n","        output = self.linear2(output)\n","        return output\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:42.391740Z","iopub.status.busy":"2022-09-29T08:30:42.391417Z","iopub.status.idle":"2022-09-29T08:30:42.404825Z","shell.execute_reply":"2022-09-29T08:30:42.403780Z","shell.execute_reply.started":"2022-09-29T08:30:42.391710Z"},"trusted":true},"outputs":[],"source":["MAX_INPUT_LENGTH = 512\n","\n","def convert_to_int(tok_ids):\n","    tok_ids = [int(x) for x in tok_ids]\n","    return tok_ids\n","\n","def split_and_cut(sentence):\n","    tokens = sentence.strip().split(\" \")\n","    tokens = tokens[:MAX_INPUT_LENGTH]\n","    return tokens\n","\n","def save_logs(logs, NUM_EPOCHS):\n","    time_now = datetime.datetime.now()\n","    time_info = f'{time_now.year}-{time_now.month}-{time_now.day}_{time_now.hour}-{time_now.minute}-{time_now.second}'\n","\n","    save_logs_path = './logs/' + str(time_info)\n","    if not os.path.exists(save_logs_path):\n","        os.makedirs(save_logs_path)\n","\n","    save_path_loss = save_logs_path  + '/loss' + '.jpg'\n","    save_path_acurracy = save_logs_path  + '/accuracy' + '.jpg'\n","\n","    x = [num for num in range(NUM_EPOCHS)]\n","    epoch_train_losses = logs[0].tolist()\n","    epoch_eval_losses = logs[1].tolist()\n","    epoch_train_accuracies = logs[2].tolist()\n","    epoch_eval_accuracies = logs[3].tolist()\n","\n","    # Plot Loss\n","    plt.plot(x, epoch_train_losses, color='red', label='Train Loss')\n","    plt.plot(x, epoch_eval_losses, color='blue', label='Eval Loss')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='upper right')\n","    plt.grid()\n","\n","    plt.savefig(save_path_loss)\n","    plt.clf()\n","\n","    # Plot Accuracy\n","    plt.plot(x, epoch_train_accuracies, color='red', label='Train Accuracy')\n","    plt.plot(x, epoch_eval_accuracies, color='blue', label='Eval Accuracy')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.grid()\n"," \n","    plt.savefig(save_path_acurracy)\n","    \n","    return None"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T09:26:57.709850Z","iopub.status.busy":"2022-09-29T09:26:57.709490Z","iopub.status.idle":"2022-09-29T09:26:57.758760Z","shell.execute_reply":"2022-09-29T09:26:57.757779Z","shell.execute_reply.started":"2022-09-29T09:26:57.709815Z"},"trusted":true},"outputs":[],"source":["class BERTNLITrainer:\n","\n","    BATCH_SIZE = 16\n","    # HIDDEN_DIM = 512\n","    DROPOUT_RATE = 0.3\n","    OUTPUT_DIM = 3\n","    LEARNING_RATE = 2e-5\n","    WEIGHT_DECAY = 0.001\n","    EPSILON = 1e-6\n","    WARMUP_PERCENT = 0.2\n","    NUM_EPOCHS = 30\n","    SAVE_PATH_MODEL = './weight'\n","    PATIENCE = 10\n","    best_val_loss = None\n","    counter = 0\n","\n","    def __init__(self):\n","        super(BERTNLITrainer, self).__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","        self.label_data = data.LabelField()\n","        self.text_data = data.Field(batch_first = True,\n","                        use_vocab = False,\n","                        tokenize = split_and_cut,\n","                        preprocessing = self.tokenizer.convert_tokens_to_ids,\n","                        pad_token = self.tokenizer.pad_token_id,\n","                        unk_token = self.tokenizer.unk_token_id)\n","        self.attention_data = data.Field(batch_first = True,\n","                        use_vocab = False,\n","                        tokenize = split_and_cut,\n","                        preprocessing = convert_to_int,\n","                        pad_token = self.tokenizer.pad_token_id)\n","        self.token_type_data = data.Field(batch_first = True,\n","                        use_vocab = False,\n","                        tokenize = split_and_cut,\n","                        preprocessing = convert_to_int,\n","                        pad_token = 1)\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    def setup_data_training(self):\n","        fields = [\n","            ('label', self.label_data), \\\n","            ('sequence', self.text_data), \\\n","            ('attention_mask', self.attention_data), \\\n","            ('token_type', self.token_type_data) \\\n","        ]\n","        self.train_data, self.valid_data = data.TabularDataset.splits(\n","            path='../input/contradictory-my-dear-watson-feature-engineering', \n","            train='train_feature_engineering.csv',\n","            validation ='valid_feature_engineering.csv',\n","            # test='test_feature_engineering.csv',\n","            format='csv',\n","            fields=fields,\n","            skip_header=True\n","        )\n","\n","        self.label_data.build_vocab(self.train_data)\n","        self.train_iterator = data.BucketIterator(\n","            (self.train_data), \n","            batch_size = self.BATCH_SIZE,\n","            sort_key = lambda x: len(x.sequence),\n","            sort=False,\n","            shuffle=True,\n","            sort_within_batch = False, \n","            device = self.device)\n","        \n","        self.valid_iterator = data.BucketIterator(\n","            (self.valid_data), \n","            batch_size = self.BATCH_SIZE,\n","            sort_key = lambda x: len(x.sequence),\n","            sort=False,\n","            shuffle=False,\n","            sort_within_batch = False, \n","            device = self.device)\n","        return self.train_iterator, self.valid_iterator\n","\n","    def setup_data_inference(self):\n","        fields = [\n","            ('sequence', self.text_data), \\\n","            ('attention_mask', self.attention_data), \\\n","            ('token_type', self.token_type_data) \\\n","            ]\n","        self.test_data = data.TabularDataset.splits(\n","            path='/kaggle/working',\n","            test='test_feature_engineering.csv',\n","            format='csv',\n","            fields=fields,\n","            skip_header=True\n","        )[0]\n","\n","        self.test_iterator = data.BucketIterator(\n","            (self.test_data), \n","            batch_size = self.BATCH_SIZE,\n","            sort_key = lambda x: len(x.sequence),\n","            sort=False,\n","            shuffle=False,\n","            sort_within_batch = False, \n","            device = self.device)\n","        return self.test_iterator\n","\n","    def __len__(self):\n","        print(f\"Number of training data: {len(self.train_data)}\")\n","        print(f\"Number of validation data: {len(self.valid_data)}\")\n","        # print(f\"Number of testing data: {len(self.test_data)}\")\n","        return len(self.train_data), len(self.valid_data)# , len(self.test_data)\n","\n","    def compute_accuracy(self, preds, ground_truth):\n","        max_preds = preds.argmax(dim = 1, keepdim = True)\n","        correct = (max_preds.squeeze(1)==ground_truth).float()\n","        return correct.sum() / len(ground_truth)\n","\n","    def epoch_time(self, start_time, end_time):\n","        elapsed_time = end_time - start_time\n","        elapsed_mins = int(elapsed_time / 60)\n","        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","        return elapsed_mins, elapsed_secs\n","\n","    def setup_training(self):\n","        self.model = BERTNLIModel(self.OUTPUT_DIM, self.DROPOUT_RATE)\n","        self.model.count_parameters()\n","        self.model.to(self.device)\n","        self.criterion = nn.CrossEntropyLoss().to(self.device)\n","        # self.optimizer = optim.Adam(self.model.parameters(), lr=self.LEARNING_RATE, eps=self.EPSILON)\n","        self.optimizer = AdamW(self.model.parameters(), lr=self.LEARNING_RATE, weight_decay=self.WEIGHT_DECAY, eps=self.EPSILON, correct_bias=False)\n","        total_steps = math.ceil(self.NUM_EPOCHS*len(self.train_data)*1./self.BATCH_SIZE)\n","        warmup_steps = int(total_steps*self.WARMUP_PERCENT)\n","        self.scheduler = get_constant_schedule_with_warmup(self.optimizer, num_warmup_steps=warmup_steps)\n","        return self.model, self.criterion, self.optimizer, self.scheduler\n","    \n","    def early_stopping(self, val_loss, model):\n","        current_loss = val_loss\n","        early_stopping = False\n","        \n","        if self.best_val_loss is None:\n","            self.best_val_loss = current_loss\n","            if not os.path.exists(self.SAVE_PATH_MODEL):\n","                os.makedirs(self.SAVE_PATH_MODEL)\n","            torch.save(model.state_dict(), self.SAVE_PATH_MODEL + '/bert-nli.pt')\n","        elif current_loss > self.best_val_loss:\n","            self.counter += 1\n","            print(f'[Early Stopping Counter]: {self.counter} out of {self.PATIENCE}')\n","            if self.counter >= self.PATIENCE:\n","                early_stopping = True\n","        else:\n","            self.best_val_loss = current_loss\n","            if not os.path.exists(self.SAVE_PATH_MODEL):\n","                os.makedirs(self.SAVE_PATH_MODEL)\n","            torch.save(model.state_dict(), self.SAVE_PATH_MODEL + '/bert-nli.pt')\n","            self.counter = 0\n","        return early_stopping\n","\n","    def train(self, model, iterator, criterion, optimizer, scheduler):\n","        epoch_loss = 0\n","        epoch_accuracy = 0\n","        model.train()\n","        \n","        for batch in iterator:\n","            optimizer.zero_grad()\n","            torch.cuda.empty_cache()\n","            \n","            sequence = batch.sequence\n","            attn_mask = batch.attention_mask\n","            token_type = batch.token_type\n","            label = batch.label\n","            \n","            predictions = model(sequence, attn_mask, token_type)\n","            loss = criterion(predictions, label)\n","            accuracy = self.compute_accuracy(predictions, label)\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            \n","            epoch_loss += loss.item()\n","            epoch_accuracy += accuracy.item()\n","        return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n","    \n","    def evaluate(self, model, iterator, criterion):\n","        epoch_loss = 0\n","        epoch_accuracy = 0\n","        model.eval()\n","        \n","        with torch.no_grad():\n","            for batch in iterator:\n","                sequence = batch.sequence\n","                attn_mask = batch.attention_mask\n","                token_type = batch.token_type\n","                labels = batch.label\n","                            \n","                predictions = model(sequence, attn_mask, token_type)\n","                loss = criterion(predictions, labels)\n","                accuracy = self.compute_accuracy(predictions, labels)\n","                \n","                epoch_loss += loss.item()\n","                epoch_accuracy += accuracy.item()\n","        return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n","    \n","    def predict_submission(self, iterator):\n","        self.model = BERTNLIModel(self.OUTPUT_DIM, self.DROPOUT_RATE)\n","        self.model.to(self.device)\n","        self.model.load_state_dict(torch.load('../input/bert-nli/bert-nli.pt'))\n","        self.model.eval()\n","        predictions = []\n","        df_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')\n","        with torch.no_grad():\n","            for batch in iterator:\n","                sequence = batch.sequence\n","                attn_mask = batch.attention_mask\n","                token_type = batch.token_type\n","                            \n","                prediction = self.model(sequence, attn_mask, token_type)\n","                _, prediction = torch.max(prediction, dim=1)\n","                prediction = prediction.flatten().tolist()\n","                predictions += prediction\n","        \n","        df_submission['prediction'] = predictions\n","        df_submission.to_csv('submission.csv', index=False)\n","    \n","    def predict_submission_df(self):\n","        df_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')\n","        df_test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\n","        tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","        self.model = BERTNLIModel(self.OUTPUT_DIM, self.DROPOUT_RATE)\n","        self.model.to(self.device)\n","        self.model.load_state_dict(torch.load('../input/bert-nli/bert-nli.pt'))\n","        self.model.eval()\n","        predictions = []\n","        premises = df_test['premise'].to_list()\n","        hypothesises = df_test['hypothesis'].to_list()\n","        \n","        for i in range(len(premises)):\n","            premise = '[CLS] ' + premises[i] + ' [SEP] '\n","            hypothesis = hypothesises[i] + ' [SEP]'\n","\n","            premise_token = tokenizer.tokenize(premise)\n","            hypothesis_token = tokenizer.tokenize(hypothesis)\n","\n","            premise_type = [0] * len(premise_token)\n","            hypothesis_type = [1] * len(hypothesis_token)\n","\n","            indexes = premise_token + hypothesis_token\n","            indexes = tokenizer.convert_tokens_to_ids(indexes)\n","            indexes_type = premise_type + hypothesis_type\n","            attn_mask = [1] * len(indexes)\n","            indexes = torch.LongTensor(indexes).unsqueeze(0).to(self.device)\n","            indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(self.device)\n","            attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(self.device)\n","\n","            prediction = self.model(indexes, attn_mask, indexes_type)\n","            prediction = prediction.argmax(dim=-1).item()\n","            predictions.append(prediction)\n","        \n","        df_submission['prediction'] = predictions\n","        df_submission.to_csv('submission.csv', index=False)\n","    \n","    def predict_inference(self, premise, hypothesis):\n","        tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","        label = ['entailment', 'neutral', 'contradiction']\n","        self.model.load_state_dict(torch.load('../input/bert-nli/bert-nli.pt'))\n","        self.model.eval()\n","        \n","        premise = '[CLS] ' + premise + ' [SEP] '\n","        hypothesis = hypothesis + ' [SEP]'\n","\n","        premise_token = tokenizer.tokenize(premise)\n","        hypothesis_token = tokenizer.tokenize(hypothesis)\n","\n","        premise_type = [0] * len(premise_token)\n","        hypothesis_type = [1] * len(hypothesis_token)\n","\n","        indexes = premise_token + hypothesis_token\n","        indexes = tokenizer.convert_tokens_to_ids(indexes)\n","        indexes_type = premise_type + hypothesis_type\n","        attn_mask = [1] * len(indexes)\n","        indexes = torch.LongTensor(indexes).unsqueeze(0).to(self.device)\n","        indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(self.device)\n","        attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(self.device)\n","        \n","        prediction = self.model(indexes, attn_mask, indexes_type)\n","        prediction = prediction.argmax(dim=-1).item()\n","        return label[prediction]\n","\n","    def train_process(self, model, train_iterator, valid_iterator, optimizer, criterion, scheduler):\n","        start_time = time.time()\n","        early_stopping = False\n","        \n","        # For progress record.\n","        train_loss_logs = np.zeros(shape=self.NUM_EPOCHS, dtype=np.float)\n","        eval_loss_logs = np.zeros(shape=self.NUM_EPOCHS, dtype=np.float)\n","        train_accuracy_logs = np.zeros(shape=self.NUM_EPOCHS, dtype=np.float)\n","        eval_accuracy_logs = np.zeros(shape=self.NUM_EPOCHS, dtype=np.float)\n","\n","        for epoch in tqdm(range(self.NUM_EPOCHS)):\n","            train_loss, train_acc = self.train(model, train_iterator, criterion, optimizer, scheduler)\n","            valid_loss, valid_acc = self.evaluate(model, valid_iterator, criterion)\n","            \n","            end_time = time.time()\n","            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n","            \n","            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","            print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n","\n","            train_loss_logs[epoch] = train_loss\n","            train_accuracy_logs[epoch] = train_acc\n","            eval_loss_logs[epoch] = valid_loss\n","            eval_accuracy_logs[epoch] = valid_acc\n","            \n","            early_stopping = self.early_stopping(valid_loss, model)\n","            if early_stopping:\n","                print(\"Early stopping\")\n","                break\n","            \n","        logs = [train_loss_logs, eval_loss_logs, train_accuracy_logs, eval_accuracy_logs]\n","        save_logs(logs=logs, NUM_EPOCHS=self.NUM_EPOCHS)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:42.452010Z","iopub.status.busy":"2022-09-29T08:30:42.451638Z","iopub.status.idle":"2022-09-29T08:30:42.622023Z","shell.execute_reply":"2022-09-29T08:30:42.621090Z","shell.execute_reply.started":"2022-09-29T08:30:42.451973Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["101 102 0 100\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n","init_token_idx = tokenizer.cls_token_id\n","eos_token_idx = tokenizer.sep_token_id\n","pad_token_idx = tokenizer.pad_token_id\n","unk_token_idx = tokenizer.unk_token_id\n","\n","print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T08:30:42.624764Z","iopub.status.busy":"2022-09-29T08:30:42.624176Z","iopub.status.idle":"2022-09-29T09:15:37.082546Z","shell.execute_reply":"2022-09-29T09:15:37.081643Z","shell.execute_reply.started":"2022-09-29T08:30:42.624719Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[########################################] | 100% Completed | 23min 19.9s\n","[########################################] | 100% Completed | 21min 23.5s\n"]}],"source":["df_train_path = \"../input/contradictory-my-dear-watson/train.csv\"\n","feature_engineering_train = FeatureEngineeringNLI(df_train_path, is_train=True)\n","feature_engineering_train.process_data()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-09-29T09:27:01.234311Z","iopub.status.busy":"2022-09-29T09:27:01.233974Z","iopub.status.idle":"2022-09-29T10:00:43.971480Z","shell.execute_reply":"2022-09-29T10:00:43.969523Z","shell.execute_reply.started":"2022-09-29T09:27:01.234280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<torchtext.legacy.data.dataset.TabularDataset object at 0x7f8c4df25c50>\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/30 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["The model has 177,853,440 trainable parameters\n","Epoch: 01 | Epoch Time: 2m 13s\n","\tTrain Loss: 1.521 | Train Acc: 37.72%\n","\tValid Loss: 1.206 | Valid Acc: 54.11%\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 1/30 [02:15<1:05:25, 135.35s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 02 | Epoch Time: 4m 28s\n","\tTrain Loss: 1.033 | Train Acc: 56.64%\n","\tValid Loss: 0.919 | Valid Acc: 60.44%\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 2/30 [04:30<1:03:14, 135.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 03 | Epoch Time: 6m 44s\n","\tTrain Loss: 0.840 | Train Acc: 64.98%\n","\tValid Loss: 0.880 | Valid Acc: 64.60%\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 3/30 [06:47<1:01:07, 135.82s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 04 | Epoch Time: 9m 1s\n","\tTrain Loss: 0.699 | Train Acc: 71.32%\n","\tValid Loss: 0.800 | Valid Acc: 68.09%\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 4/30 [09:03<58:55, 136.00s/it]  "]},{"name":"stdout","output_type":"stream","text":["Epoch: 05 | Epoch Time: 11m 17s\n","\tTrain Loss: 0.567 | Train Acc: 77.83%\n","\tValid Loss: 0.903 | Valid Acc: 68.71%\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 5/30 [11:19<56:40, 136.02s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 06 | Epoch Time: 13m 33s\n","\tTrain Loss: 0.458 | Train Acc: 82.55%\n","\tValid Loss: 1.056 | Valid Acc: 66.37%\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 6/30 [13:35<54:25, 136.05s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 07 | Epoch Time: 15m 49s\n","\tTrain Loss: 0.343 | Train Acc: 87.60%\n","\tValid Loss: 1.154 | Valid Acc: 65.87%\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 7/30 [15:51<52:06, 135.92s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 08 | Epoch Time: 18m 4s\n","\tTrain Loss: 0.247 | Train Acc: 91.40%\n","\tValid Loss: 1.153 | Valid Acc: 67.02%\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 8/30 [18:06<49:47, 135.80s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 09 | Epoch Time: 20m 20s\n","\tTrain Loss: 0.184 | Train Acc: 93.92%\n","\tValid Loss: 1.294 | Valid Acc: 67.93%\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 9/30 [20:23<47:34, 135.93s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10 | Epoch Time: 22m 36s\n","\tTrain Loss: 0.160 | Train Acc: 94.31%\n","\tValid Loss: 1.367 | Valid Acc: 67.39%\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 10/30 [22:39<45:19, 135.95s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11 | Epoch Time: 24m 52s\n","\tTrain Loss: 0.130 | Train Acc: 95.69%\n","\tValid Loss: 1.427 | Valid Acc: 68.83%\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 11/30 [24:54<43:01, 135.87s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12 | Epoch Time: 27m 8s\n","\tTrain Loss: 0.108 | Train Acc: 96.42%\n","\tValid Loss: 1.482 | Valid Acc: 68.46%\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 12/30 [27:10<40:46, 135.90s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 13 | Epoch Time: 29m 24s\n","\tTrain Loss: 0.092 | Train Acc: 96.97%\n","\tValid Loss: 1.469 | Valid Acc: 67.31%\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 13/30 [29:26<38:28, 135.82s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 14 | Epoch Time: 31m 40s\n","\tTrain Loss: 0.074 | Train Acc: 97.55%\n","\tValid Loss: 1.612 | Valid Acc: 67.02%\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 14/30 [33:37<38:25, 144.10s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e4c61afa9d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_data_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-28f85b2b6cd6>\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(self, model, train_iterator, valid_iterator, optimizer, criterion, scheduler)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-28f85b2b6cd6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, iterator, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer = BERTNLITrainer()\n","train_iterator, valid_iterator = trainer.setup_data_training()\n","model, criterion, optimizer, scheduler = trainer.setup_training()\n","trainer.train_process(model, train_iterator, valid_iterator, optimizer, criterion, scheduler)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:18:07.837045Z","iopub.status.idle":"2022-09-29T09:18:07.839165Z"},"trusted":true},"outputs":[],"source":["\"\"\"df_test_path = \"../input/contradictory-my-dear-watson/test.csv\"\n","feature_engineering_test = FeatureEngineeringNLI(df_test_path, is_train=False)\n","feature_engineering_test.process_data()\n","predictor = BERTNLITrainer()\n","test_iterator = predictor.setup_data_inference()\n","predictor.predict_submission(test_iterator)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-09-29T09:18:07.840953Z","iopub.status.idle":"2022-09-29T09:18:07.842167Z"},"trusted":true},"outputs":[],"source":["!zip -r -j /kaggle/working/output.zip ./"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('feature-engineering')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"b20fa969a1caffde07c0ffe48260125c28ddbcaf9a3d96b52c3fb6f9a671671e"}}},"nbformat":4,"nbformat_minor":4}
